name: Stress Test on Pre/RC Release

on:
  push:
    tags:
      - 'v*-rc*'  # Release candidates
      - 'v*-pre*' # Pre-releases
  workflow_dispatch:
    inputs:
      tag:
        description: 'Release tag to test (e.g., v1.0.0-rc1, v1.0.0-pre1)'
        required: true
        type: string

permissions:
  contents: read
  id-token: write

env:
  AWS_REGION: us-east-1
  EKS_CLUSTER_NAME: odigos-stress-test
  NAMESPACE: load-test

jobs:
  stress-test:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
      - name: Checkout
        uses: actions/checkout@v5

      - name: Determine Release Tag
        id: tag
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "tag=${{ github.event.inputs.tag }}" >> $GITHUB_OUTPUT
          else
            echo "tag=${GITHUB_REF#refs/tags/}" >> $GITHUB_OUTPUT
          fi
          echo "Release tag: ${{ steps.tag.outputs.tag }}"

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.32.0'

      - name: Install Helm
        uses: azure/setup-helm@v4
        with:
          version: 'v3.15.2'

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}

      - name: Verify cluster connection
        run: |
          kubectl get nodes
          kubectl get namespaces

      - name: Install/Upgrade Odigos via Helm
        run: |
          helm repo add odigos https://odigos-io.github.io/odigos
          helm repo update
          helm upgrade --install odigos odigos/odigos --version ${{ steps.tag.outputs.tag }} --namespace odigos-system --create-namespace --wait

      - name: Wait for Odigos to be ready
        run: |
          kubectl wait --for=condition=available --timeout=300s deployment/odigos-scheduler -n odigos-system

      - name: Create load-test namespace
        run: |
          kubectl create namespace ${{ env.NAMESPACE }} --dry-run=client -o yaml | kubectl apply -f -

      - name: Deploy Load Test Applications
        run: |
          # Deploy all span generators using the manifest file (with default replica counts)
          kubectl apply -f stress-test/deploy/workloads/span-generators.yaml
          
          # Scale up all generators to 7 replicas for stress testing
          kubectl scale deployment span-go --replicas=7 -n ${{ env.NAMESPACE }}
          kubectl scale deployment span-java --replicas=7 -n ${{ env.NAMESPACE }}
          kubectl scale deployment span-node --replicas=7 -n ${{ env.NAMESPACE }}
          kubectl scale deployment span-python --replicas=7 -n ${{ env.NAMESPACE }}
          kubectl scale deployment span-dotnet --replicas=7 -n ${{ env.NAMESPACE }}
          
          echo "All generators scaled up to 7 replicas for stress testing"
          
          # Deploy a Job that will scale down after 2 hours
          kubectl apply -f stress-test/deploy/workloads/scale-down-job.yaml
          echo "Scale-down job deployed - will scale generators to 0 after 2 hours"

      - name: Wait for load test apps to be ready
        run: |
          kubectl wait --for=condition=available --timeout=300s deployment/span-go -n ${{ env.NAMESPACE }}
          kubectl wait --for=condition=available --timeout=300s deployment/span-java -n ${{ env.NAMESPACE }}
          kubectl wait --for=condition=available --timeout=300s deployment/span-node -n ${{ env.NAMESPACE }}
          kubectl wait --for=condition=available --timeout=300s deployment/span-python -n ${{ env.NAMESPACE }}
          kubectl wait --for=condition=available --timeout=300s deployment/span-dotnet -n ${{ env.NAMESPACE }}

      - name: Get cluster information
        id: cluster-info
        run: |
          # Get cluster endpoint
          CLUSTER_ENDPOINT=$(aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }} --query 'cluster.endpoint' --output text)
          echo "cluster_endpoint=$CLUSTER_ENDPOINT" >> $GITHUB_OUTPUT
          
          # Get cluster region
          echo "cluster_region=${{ env.AWS_REGION }}" >> $GITHUB_OUTPUT
          
          # Get cluster name
          echo "cluster_name=${{ env.EKS_CLUSTER_NAME }}" >> $GITHUB_OUTPUT
          
          # Get EC2 instance ID for port forwarding
          echo "Looking for EC2 instances in cluster: ${{ env.EKS_CLUSTER_NAME }}"
          INSTANCE_ID=$(aws ec2 describe-instances --region ${{ env.AWS_REGION }} --filters "Name=tag:kubernetes.io/cluster/${{ env.EKS_CLUSTER_NAME }},Values=owned" "Name=instance-state-name,Values=running" --query 'Reservations[0].Instances[0].InstanceId' --output text)
          echo "First attempt - Instance ID: $INSTANCE_ID"
          
          if [ "$INSTANCE_ID" = "None" ] || [ -z "$INSTANCE_ID" ]; then
            echo "Trying fallback method..."
            # Fallback: try to get any running instance in the cluster
            INSTANCE_ID=$(aws ec2 describe-instances --region ${{ env.AWS_REGION }} --filters "Name=instance-state-name,Values=running" --query 'Reservations[0].Instances[0].InstanceId' --output text)
            echo "Fallback - Instance ID: $INSTANCE_ID"
          fi
          
          # If still no instance found, set a placeholder
          if [ "$INSTANCE_ID" = "None" ] || [ -z "$INSTANCE_ID" ]; then
            INSTANCE_ID="INSTANCE_ID_NOT_FOUND"
            echo "Warning: No EC2 instance found for port forwarding"
          fi
          
          echo "instance_id=$INSTANCE_ID" >> $GITHUB_OUTPUT

      - name: Get Grafana dashboard URL
        id: grafana-info
        run: |
          # Try to get Grafana service external IP or use port-forward instructions
          GRAFANA_IP=$(kubectl get svc -n monitoring grafana -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")
          GRAFANA_NODEPORT=$(kubectl get svc -n monitoring grafana -o jsonpath='{.spec.ports[0].nodePort}' 2>/dev/null || echo "")
          
          if [ -n "$GRAFANA_IP" ]; then
            echo "grafana_url=http://$GRAFANA_IP:3000" >> $GITHUB_OUTPUT
          elif [ -n "$GRAFANA_NODEPORT" ]; then
            echo "grafana_url=http://<NODE_IP>:$GRAFANA_NODEPORT" >> $GITHUB_OUTPUT
          else
            echo "grafana_url=Use: kubectl port-forward -n monitoring svc/grafana 3000:3000" >> $GITHUB_OUTPUT
          fi

      - name: Send Slack notification - Success
        if: success()
        uses: odigos-io/ci-core/.github/actions/slack-release-notification@main
        with:
          webhook-url: ${{ secrets.ODIGOS_RELEASE_STATUS_WEBHOOK_URL }}
          success-description: "üöÄ Stress Test Completed Successfully! Release: ${{ steps.tag.outputs.tag }} | Cluster: ${{ steps.cluster-info.outputs.cluster_name }} | Region: ${{ steps.cluster-info.outputs.cluster_region }} | Load Test Apps: Go(7), Java(7), Node(7), Python(7), .NET(7) | Connect: aws eks update-kubeconfig --region ${{ steps.cluster-info.outputs.cluster_region }} --name ${{ steps.cluster-info.outputs.cluster_name }} | Port Forward: ClickHouse(8123): aws ssm start-session --target ${{ steps.cluster-info.outputs.instance_id }} --document-name AWS-StartPortForwardingSession --parameters '{\"portNumber\":[\"8123\"],\"localPortNumber\":[\"8123\"]}' | Prometheus(9090): aws ssm start-session --target ${{ steps.cluster-info.outputs.instance_id }} --document-name AWS-StartPortForwardingSession --parameters '{\"portNumber\":[\"9090\"],\"localPortNumber\":[\"9090\"]}' | Grafana(3000): aws ssm start-session --target ${{ steps.cluster-info.outputs.instance_id }} --document-name AWS-StartPortForwardingSession --parameters '{\"portNumber\":[\"3000\"],\"localPortNumber\":[\"3000\"]}'"
          failure-description: "‚ùå Stress Test Failed! Release: ${{ steps.tag.outputs.tag }} | Cluster: ${{ steps.cluster-info.outputs.cluster_name }} | Region: ${{ steps.cluster-info.outputs.cluster_region }} | Check GitHub Actions logs for details | Connect: aws eks update-kubeconfig --region ${{ steps.cluster-info.outputs.cluster_region }} --name ${{ steps.cluster-info.outputs.cluster_name }} | Port Forward: ClickHouse(8123): aws ssm start-session --target ${{ steps.cluster-info.outputs.instance_id }} --document-name AWS-StartPortForwardingSession --parameters '{\"portNumber\":[\"8123\"],\"localPortNumber\":[\"8123\"]}' | Prometheus(9090): aws ssm start-session --target ${{ steps.cluster-info.outputs.instance_id }} --document-name AWS-StartPortForwardingSession --parameters '{\"portNumber\":[\"9090\"],\"localPortNumber\":[\"9090\"]}' | Grafana(3000): aws ssm start-session --target ${{ steps.cluster-info.outputs.instance_id }} --document-name AWS-StartPortForwardingSession --parameters '{\"portNumber\":[\"3000\"],\"localPortNumber\":[\"3000\"]}'"
          tag: ${{ steps.tag.outputs.tag }}

      - name: Send Slack notification - Failure
        if: failure()
        uses: odigos-io/ci-core/.github/actions/slack-release-notification@main
        with:
          webhook-url: ${{ secrets.ODIGOS_RELEASE_STATUS_WEBHOOK_URL }}
          success-description: "üöÄ Stress Test Completed Successfully! Release: ${{ steps.tag.outputs.tag }} | Cluster: ${{ steps.cluster-info.outputs.cluster_name }} | Region: ${{ steps.cluster-info.outputs.cluster_region }} | Load Test Apps: Go(7), Java(7), Node(7), Python(7), .NET(7) | Connect: aws eks update-kubeconfig --region ${{ steps.cluster-info.outputs.cluster_region }} --name ${{ steps.cluster-info.outputs.cluster_name }} | Port Forward: ClickHouse(8123): aws ssm start-session --target ${{ steps.cluster-info.outputs.instance_id }} --document-name AWS-StartPortForwardingSession --parameters '{\"portNumber\":[\"8123\"],\"localPortNumber\":[\"8123\"]}' | Prometheus(9090): aws ssm start-session --target ${{ steps.cluster-info.outputs.instance_id }} --document-name AWS-StartPortForwardingSession --parameters '{\"portNumber\":[\"9090\"],\"localPortNumber\":[\"9090\"]}' | Grafana(3000): aws ssm start-session --target ${{ steps.cluster-info.outputs.instance_id }} --document-name AWS-StartPortForwardingSession --parameters '{\"portNumber\":[\"3000\"],\"localPortNumber\":[\"3000\"]}'"
          failure-description: "‚ùå Stress Test Failed! Release: ${{ steps.tag.outputs.tag }} | Cluster: ${{ steps.cluster-info.outputs.cluster_name }} | Region: ${{ steps.cluster-info.outputs.cluster_region }} | Check GitHub Actions logs for details | Connect: aws eks update-kubeconfig --region ${{ steps.cluster-info.outputs.cluster_region }} --name ${{ steps.cluster-info.outputs.cluster_name }} | Port Forward: ClickHouse(8123): aws ssm start-session --target ${{ steps.cluster-info.outputs.instance_id }} --document-name AWS-StartPortForwardingSession --parameters '{\"portNumber\":[\"8123\"],\"localPortNumber\":[\"8123\"]}' | Prometheus(9090): aws ssm start-session --target ${{ steps.cluster-info.outputs.instance_id }} --document-name AWS-StartPortForwardingSession --parameters '{\"portNumber\":[\"9090\"],\"localPortNumber\":[\"9090\"]}' | Grafana(3000): aws ssm start-session --target ${{ steps.cluster-info.outputs.instance_id }} --document-name AWS-StartPortForwardingSession --parameters '{\"portNumber\":[\"3000\"],\"localPortNumber\":[\"3000\"]}'"
          tag: ${{ steps.tag.outputs.tag }}


